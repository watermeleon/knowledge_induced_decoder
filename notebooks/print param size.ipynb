{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2cb06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c34192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import skimage.io as io\n",
    "import PIL.Image\n",
    "from IPython.display import Image \n",
    "\n",
    "from knowgraph_conceptnet import KnowledgeGraph\n",
    "from data.utils import *\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModel, CLIPModel\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06e6b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import Transformer, MemoryAugmentedEncoder, PromptDecoder, ScaledDotProductAttentionMemory, MultiLevelEncoder, ScaledDotProductAttention, VanillaDecoder, ParallelPromptDecoder , StackedPromptDecoder\n",
    "from knowgraph_conceptnet import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211ed97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162adad8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18260/911576860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                      attention_module_kwargs={'m': 0}, dropout=0.1, d_model = 512, h=8)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mseg_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg_token\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mknowledge_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKnowledgeGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizerBW_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_select\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_select\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_keywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrw_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_relatedwords\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0menc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_kw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_kw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_rel_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_l2r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_only_l2r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_faiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_faiss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrc_posidx2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrc_posidx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcn_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = MemoryAugmentedEncoder(1, 0, d_in=728,  attention_module=ScaledDotProductAttention,\n",
    "                                     attention_module_kwargs={'m': 0}, dropout=0.1, d_model = 512, h=8)\n",
    "\n",
    "# seg_token = args.seg_token == \"True\"\n",
    "\n",
    "# knowledge_graph = KnowledgeGraph(transform_tok = tokenizerBW_clip, device = device, edge_select=args.edge_select, spec = spec, kw_size = args.num_keywords, rw_size = args.num_relatedwords , enc_model = args.enc_model, only_kw=args.only_kw, norel= args.no_rel_label, only_l2r = args.rel_only_l2r, use_faiss = args.use_faiss, rc_posidx2 =args.rc_posidx2, cn_version=args.cn_version)\n",
    "knowledge_graph= None\n",
    "max_inp_seq= 128\n",
    "if args.decoder == \"kg_infused\":\n",
    "    print(\"using normal dec\")\n",
    "    decoder = PromptDecoder(len(tokenizerBW), max_inp_seq, args.N_dec, spec['pad_tokenid'],h=args.head, seg_token= seg_token, KG = knowledge_graph , enc_model= args.enc_model, spec=spec, pt_tokemb=args.pt_token_emb, dropout=args.dropout, d_model = args.d_model, seg_param=args.seg_param)\n",
    "elif args.decoder == \"parallel\":\n",
    "    print(\"using parallel dec\")\n",
    "    decoder = ParallelPromptDecoder(len(tokenizerBW), max_inp_seq, args.N_dec, spec['pad_tokenid'], h=args.head, seg_token= seg_token, KG = knowledge_graph , enc_model= args.enc_model, spec=spec, pt_tokemb=args.pt_token_emb, dropout=args.dropout, d_model = args.d_model, pll_dec_type = args.pll_dec, seg_token_kw = args.seg_token_kw, seg_param=args.seg_param)\n",
    "elif args.decoder == \"stacked\":\n",
    "    print(\"using stacked decoder\")\n",
    "    decoder = StackedPromptDecoder(len(tokenizerBW), max_inp_seq, args.N_dec, spec['pad_tokenid'], h=args.head, seg_token= seg_token, KG = knowledge_graph , enc_model= args.enc_model, spec=spec, pt_tokemb=args.pt_token_emb, dropout=args.dropout, one_kw_token=args.one_kw_token, d_model = args.d_model, seg_token_kw = args.seg_token_kw, use_gpt=args.stck_gpt2, seg_param=args.seg_param)\n",
    "elif args.decoder == \"vanilla\":\n",
    "   print(\"using vanilla decoder\")\n",
    "   decoder = VanillaDecoder(len(tokenizerBW), max_inp_seq, args.N_dec, spec['pad_tokenid'], h=args.head, enc_model = args.enc_model, dropout=args.dropout, d_model = args.d_model, seg_token_kw = args.seg_token_kw, seg_param=args.seg_param)\n",
    "\n",
    "model = Transformer(spec['bos_tokenid'], encoder, decoder).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38143b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_prefix_caption",
   "language": "python",
   "name": "clip_prefix_caption"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
