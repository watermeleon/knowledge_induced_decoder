{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0391bd",
   "metadata": {},
   "source": [
    "# Note: currently not using names to filter the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3351b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "import copy\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import clip\n",
    "import pickle\n",
    "import torch\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c78e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c62346",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data_files/openwebtext-clean2.num') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aadfce",
   "metadata": {},
   "source": [
    "### Get names from predefines text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ca8da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8842\n"
     ]
    }
   ],
   "source": [
    "# get all names (first and last) in a list:\n",
    "with open('../random-name/first-names.txt') as f:\n",
    "    namelines = f.readlines()\n",
    "with open('../random-name/middle-names.txt') as f:\n",
    "    mnamelines = f.readlines()\n",
    "name_list = []\n",
    "for line in namelines:\n",
    "    name_list.append(line[:-1].lower())\n",
    "for line in mnamelines:\n",
    "    name_list.append(line[:-1].lower())\n",
    "print(len(name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f55f34e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092966\n",
      "2083622\n"
     ]
    }
   ],
   "source": [
    "# get all words in a list\n",
    "freq_list_noname = []\n",
    "freq_list = []\n",
    "name_set = set(name_list)\n",
    "\n",
    "for line in lines:\n",
    "    splitline = line.split(\"\\t\")\n",
    "    middl = list(map(int,splitline[1:3]))\n",
    "\n",
    "    newline = [splitline[0].strip(\"''\").lower()] + middl + [int(splitline[4])]    \n",
    "    ascii_word = re.sub(r'[^\\w\\s]','',newline[0])\n",
    "    if len(ascii_word) == 0 :\n",
    "        continue\n",
    "    freq_list.append(newline)\n",
    "    if newline[0] not in name_set:\n",
    "        freq_list_noname.append(newline)\n",
    "\n",
    "print(len(freq_list)) \n",
    "print(len(freq_list_noname)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13418f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 445503354, 444364108, 7958752],\n",
       " ['to', 231470564, 231003503, 7933619],\n",
       " ['p', 234522618, 228609674, 8424304],\n",
       " ['and', 205912374, 205088775, 7978227],\n",
       " ['of', 205778822, 203996377, 7706269],\n",
       " ['a', 188263522, 187046991, 7920297],\n",
       " ['in', 148481168, 147433279, 7612827],\n",
       " ['that', 103273639, 102490626, 6922986],\n",
       " ['is', 98127345, 97226904, 7285873],\n",
       " ['for', 83179488, 82242810, 7465139]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_list.sort(key=lambda row:row[2])\n",
    "freq_list.reverse()\n",
    "top100th = freq_list[:100000]\n",
    "top100th[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c4d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the ascii coversion went right\n",
    "for line in top100th:\n",
    "    ascii_word = re.sub(r'[^\\w\\s]','',line[0])\n",
    "    if len(ascii_word) == 0 :\n",
    "        print(\"hey\", line[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2131d",
   "metadata": {},
   "source": [
    "## Remove keywords with more than 4 tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff53dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  CLIPTokenizerFast\n",
    "tokenizer =  CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b0b83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100th_small = []\n",
    "for line in top100th:\n",
    "    word = line[0]\n",
    "    toks = tokenizer(word).input_ids[1:-1]\n",
    "    if len(toks)>4:\n",
    "        continue\n",
    "    top100th_small.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec5559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# save the top 10 thousand most common words\n",
    "top10th = top100th_small[:10000]\n",
    "print(len(top10th))\n",
    "f = open(\"../data_files/openwebtext_top10th.txt\", \"w\")\n",
    "f.write(str(top10th))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edd1159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 words: ['the', 'to', 'p', 'and', 'of', 'a', 'in', 'that', 'is', 'for', 'it', 'on', 'i', 'with', 'you', 'this', 'as', 's', 'be', 'are', 'was', 'have', 'not', 'at', 'by', 'but', 'from', 'we', 'he', 'they', 'h', 'or', 'an', 's', 'has', 'will', 'can', 'his', 'more', 'if', 'all', 'about', 'your', 'their', 'one', 'who', 't', 'there', 'so', 'what', 'said', 'do', 'would', 'out', 'when', 'up', 'which', 'like', 'been', 'were', 'just', 'our', 'people', 'had', 'no', 'my', 'new', 'some', 'also', 'other', 'time', 'than', 'them', 'get', 'how', 'its', \"n't\", 'into', 'only', 'after', 'now', 'any', 'because', 'me', 'over', 'could', 'these', 'first', 'she', 'even', 'us', 'most', 'her', 'make', 'then', 'two', 'years', 'may', 'see', 'many']\n"
     ]
    }
   ],
   "source": [
    "print(\"First 100 words:\", [word[0] for word in top10th[:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aed8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_prefix_caption",
   "language": "python",
   "name": "clip_prefix_caption"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
