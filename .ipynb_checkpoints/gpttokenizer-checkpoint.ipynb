{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d3cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from data.utils import *\n",
    "tokenizer = get_tokenizer(\"spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef5dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989cb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizergpt = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb636114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leon', 'is', '[', 'PAD', ']', 'cool']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7267/36649058.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# tokenizergpt(inptext)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minptext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minptext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "inptext = tokenizer(\"leon is [PAD] cool\")\n",
    "# tokenizergpt(inptext)\n",
    "print(inptext)\n",
    "outptext = tokenizer.decode(inptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34cf1236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leon', 'is', '[', 'PAD', ']', 'cool']\n"
     ]
    }
   ],
   "source": [
    "inptext = tokenizer(\"leon is [PAD] cool\")\n",
    "print(inptext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcdc694",
   "metadata": {},
   "source": [
    "## BERT tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29f89505",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerbert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6adb1013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_or_get_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_from_pretrained',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_get_repo_url_from_name',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_push_to_hub',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizerbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3012960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [49406, 9763, 533, 314, 7601, 316, 2077, 49407], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(inptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26b0b0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 6506, 102], [101, 2003, 102], [101, 1031, 102], [101, 11687, 102], [101, 1033, 102], [101, 4658, 102]], 'token_type_ids': [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerbert(inptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5484999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 6506, 2003, 0, 4658, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerbert(\"leon is [PAD] cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "942cb0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 6506, 102], [101, 2003, 102], [101, 1031, 102], [101, 11687, 102], [101, 1033, 102], [101, 4658, 102]], 'token_type_ids': [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerbert(tokenizer(\"leon is [PAD] cool\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b626f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[49408, 9763, 49409], [49408, 533, 49409], [49408, 314, 49409], [49408, 7601, 49409], [49408, 316, 49409], [49408, 2077, 49409]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerclip(tokenizer(\"leon is [PAD] cool\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cf4f576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizerbert(tokenizer(\"leon is [PAD] cool\")).input_ids))\n",
    "print(len(tokenizerclip(tokenizer(\"leon is [PAD] cool\")).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "051c221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "101\n",
      "102\n",
      "101\n",
      "now clip:\n",
      "{'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}\n",
      "49409\n",
      "49408\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizerbert.special_tokens_map)\n",
    "print(tokenizerbert.cls_token_id)\n",
    "print(tokenizerbert.sep_token_id)\n",
    "cls_tok = tokenizerbert.cls_token\n",
    "sep_tok = tokenizerbert.sep_token\n",
    "# tokenizerbert.bos_token = tokenizerbert.cls_token if \n",
    "\n",
    "\n",
    "print(tokenizerbert.bos_token_id)\n",
    "print(\"now clip:\")\n",
    "print(tokenizerclip.special_tokens_map)\n",
    "print(tokenizerclip.eos_token_id)\n",
    "print(tokenizerclip.bos_token_id)\n",
    "print(tokenizerclip.cls_token_id)\n",
    "# cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816f272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03ad0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizerclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0e293093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cls_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "49408 49409 0\n"
     ]
    }
   ],
   "source": [
    "cls_tok = tok.cls_token\n",
    "# sep_tok = tokenizerbert.sep_token\n",
    "print(cls_tok)\n",
    "\n",
    "bos_tok = tok.cls_token_id if cls_tok is not None else tok.bos_token_id\n",
    "eos_tok =  tok.sep_token_id if cls_tok is not None else tok.eos_token_id\n",
    "pad_tok = tok.pad_token_id\n",
    "print(bos_tok, eos_tok, pad_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b02bf8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_or_get_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_from_pretrained',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_get_repo_url_from_name',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_push_to_hub',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizerbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24427905",
   "metadata": {},
   "source": [
    "## CLIP tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6857c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTokenizerFast\n",
    "# from transformers import AutoTokenizer, CLIPTokenizer, CLIPTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15364d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerclip2 = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05d89b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizerclip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13132/125740127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizerclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizerclip' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizerclip.decode(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d5eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizerclip = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\", bos_token = \"[BOS]\", eos_token = \"[EOS]\", pad_token = \"[PAD]\")\n",
    "# tokenizerclip = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\", bos_token = \"[BOS]\", eos_token = \"[EOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d4250d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizerclipfast =  CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\", bos_token = \"[BOS]\",bos_token_id= 49408, eos_token = \"[EOS]\",eos_token_id = 49409 , pad_token = \"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a89958dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org: [49408, 49408, 320, 49410, 786, 2862, 530, 320, 7571, 593, 550, 17143, 49409, 256, 49409]\n",
      "fast: [49406, 49408, 320, 49410, 786, 2862, 530, 320, 7571, 593, 550, 17143, 49409, 256, 49407]\n",
      "for orig padid  0 , bosid 49408 ,eosid 49409\n",
      "for fast padid  0 , bosid 49408 ,eosid 49409\n",
      "/n for decoding:\n",
      "org: [BOS] [BOS] a  [PAD] man standing in a fieldwith an umbrella  [EOS]!  [EOS]\n",
      "fast: <|startoftext|>[BOS]a</w>[PAD]man</w>standing</w>in</w>a</w>fieldwith</w>an</w>umbrella</w>[EOS]!</w><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# tokenizerclip\n",
    "sent = '[BOS]a [PAD] man standing in a fieldwith an umbrella  [EOS]!'\n",
    "print(\"org:\", tokenizerclip(sent).input_ids)\n",
    "print(\"fast:\",tokenizerclipfast(sent).input_ids)\n",
    "print(\"for orig\",\"padid \",tokenizerclip.pad_token_id, \", bosid\",tokenizerclip.bos_token_id, \",eosid\",tokenizerclip.eos_token_id)\n",
    "print(\"for fast\",\"padid \",tokenizerclipfast.pad_token_id, \", bosid\",tokenizerclipfast.bos_token_id, \",eosid\",tokenizerclipfast.eos_token_id)\n",
    "\n",
    "print(\"/n for decoding:\")\n",
    "print(\"org:\", tokenizerclip.decode(tokenizerclip(sent).input_ids))\n",
    "print(\"fast:\",tokenizerclipfast.decode(tokenizerclipfast(sent).input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d191267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org: [49408, 320, 49410, 786, 2862, 530, 320, 1570, 593, 550, 17143, 256, 49409]\n",
      "fast: [49406, 320, 49410, 786, 2862, 530, 320, 1570, 593, 550, 17143, 256, 49407]\n",
      "for orig padid  0 , bosid 49408 ,eosid 49409\n",
      "for fast padid  0 , bosid 49408 ,eosid 49409\n",
      "/n for decoding:\n",
      "org: [BOS] a  [PAD] man standing in a field with an umbrella!  [EOS]\n",
      "fast: <|startoftext|>a man standing in a field with an umbrella! \n"
     ]
    }
   ],
   "source": [
    "# tokenizerclip\n",
    "sent = 'a [PAD] man standing in a field with an umbrella!'\n",
    "print(\"org:\", tokenizerclip(sent).input_ids)\n",
    "print(\"fast:\",tokenizerclipfast(sent).input_ids)\n",
    "print(\"for orig\",\"padid \",tokenizerclip.pad_token_id, \", bosid\",tokenizerclip.bos_token_id, \",eosid\",tokenizerclip.eos_token_id)\n",
    "print(\"for fast\",\"padid \",tokenizerclipfast.pad_token_id, \", bosid\",tokenizerclipfast.bos_token_id, \",eosid\",tokenizerclipfast.eos_token_id)\n",
    "\n",
    "print(\"/n for decoding:\")\n",
    "print(\"org:\", tokenizerclip.decode(tokenizerclip(sent).input_ids))\n",
    "print(\"fast:\",tokenizerclip.decode(tokenizerclipfast(sent).input_ids, skip_special_tokens=True))\n",
    "# .replace(\"</w>\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a016080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org: <|startoftext|>\n",
      "org: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(\"org:\", tokenizerclip.decode(49406))\n",
    "print(\"org:\", tokenizerclipfast.decode(49407))\n",
    "\n",
    "print(\"org:\", tokenizerclip.eos_token_id)\n",
    "print(\"org:\", tokenizerclipfast)\n",
    "# print(\"fast:\",tokenizerclipfast(sent).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "403995ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[49408,   320, 49410,   786,  2862,   530,   320,  1570,   593,   550,\n",
      "         17143,   256, 49409],\n",
      "        [49406,   320, 49410,   786,  2862,   530,   320,  1570,   593,   550,\n",
      "         17143,   256, 49407]])\n",
      "tensor([[49408,   320, 49410,   786,  2862,   530,   320,  1570,   593,   550,\n",
      "         17143,   256, 49409],\n",
      "        [49406,   320, 49410,   786,  2862,   530,   320,  1570,   593,   550,\n",
      "         17143,   256, 49409]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[49408, 320, 49410, 786, 2862, 530, 320, 1570, 593, 550, 17143, 256, 49409], [49406, 320, 49410, 786, 2862, 530, 320, 1570, 593, 550, 17143, 256, 49407]])\n",
    "endtexttokid = 49407\n",
    "print(49407 in a)\n",
    "if endtexttokid in a:\n",
    "    newa = torch.tensor([[tokenid if tokenid != endtexttokid else 49409 for tokenid in sent ] for sent in a])\n",
    "print(a)\n",
    "print(newa)\n",
    "# b = [list(sent).replace(49407, 49409) for sent in a]\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e569e03b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18035/4019583709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizerclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "tokenizerclip.convert_tokens_to_ids[\"[PAD]\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc7ac446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cls_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bostokn 49406 [49406, 9763, 49407] 49407\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizerclipfast._cls_token)\n",
    "sample_txt = tokenizerclipfast(\"leon\").input_ids\n",
    "print(\"bostokn\",sample_txt[0], sample_txt, sample_txt[-1])\n",
    "print(tokenizerclipfast.cls_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a43ee28a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_or_get_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_from_pretrained',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_get_repo_url_from_name',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_push_to_hub',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " 'add_prefix_space',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizerclipfast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "25f05c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', '[EOS]', '<|endoftext|>', '[PAD]'] [49408, 49409, 49407, 49410]\n",
      "[PAD]\n",
      "[PAD]\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# tokenizerclip.enable_padding()\n",
    "print(tokenizerclip.all_special_tokens, tokenizerclip.all_special_ids)\n",
    "\n",
    "\n",
    "print(tokenizerclip._pad_token)\n",
    "print(tokenizerclip.pad_token)\n",
    "\n",
    "# print(tokenizerclip._pad_token_id)\n",
    "print(tokenizerclip.pad_token_id)\n",
    "# pad_token_type_id\n",
    "\n",
    "print(tokenizerclip._pad_token_type_id)\n",
    "print(tokenizerclip.pad_token_type_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9e081c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49410"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerclip.encode(tokenizerclip.pad_token)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8ffd2379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerbert.encode(tokenizerbert.pad_token)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b129c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerclip.pad_token = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc7d8f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49408, 49410, 49409]\n",
      "[49408, 35579, 49409]\n",
      "!\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# sep_token\n",
    "print(tokenizerclip.encode(\"[PAD]\"))\n",
    "print(tokenizerclip.encode(\"!!!!!!!!!!\"))\n",
    "\n",
    "#\n",
    "print(tokenizerclip.decode([0]))\n",
    "print(tokenizerclip.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fddb567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a man standing in a fieldwith an umbrella  ', '', '', '', '!!!!!!!!!!']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'a man standing in a fieldwith an umbrella  [EOS][EOS][EOS][EOS]!!!!!!!!!!'\n",
    "b = a.split(\"[EOS]\")\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be626ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] 0\n",
      "[EOS] 49409\n",
      "[BOS] 49408\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "len(tokenizerclip)\n",
    "print(tokenizerclip.pad_token, tokenizerclip.pad_token_id)\n",
    "print(tokenizerclip.eos_token, tokenizerclip.eos_token_id)\n",
    "print(tokenizerclip.bos_token, tokenizerclip.bos_token_id)\n",
    "print(tokenizerclip.sep_token, tokenizerclip.sep_token_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94706f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[49408, 9763, 533, 49410, 2077, 49409],\n",
       " [49408, 9763, 533, 49410, 2077, 49409]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inptext = tokenizer(\"leon is [PAD] cool\")\n",
    "tokenizerclip([\"leon is [PAD] cool\", \"leon is [PAD] cool\"]).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104d154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inptext = tokenizer(\"leon is [PAD] cool\")\n",
    "tokenizergpt(inptext)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_prefix_caption",
   "language": "python",
   "name": "clip_prefix_caption"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
